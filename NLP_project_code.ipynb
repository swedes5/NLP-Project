{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79525f-da49-486a-bb29-af2794f34ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf64396-b90c-4ce3-9dc8-b2682065a255",
   "metadata": {},
   "source": [
    "INTRODUCTION:\n",
    "    \n",
    "My project is about analyzing tweets by using a workflow that looks something like this: \n",
    "read in data -->  clean text data (regex/nltk) --> tokenize words -->  \n",
    "reduce dimensions (sklearn SVD, sklearn PCA, sklearn TFIDFVectorizer) -->  KMeans clustering --> Check Topics from clusters from most common words.\n",
    "\n",
    "\n",
    "The goal of this project was to use unsupervised learning techniques to attempt to draw some meaning from a huge corpus made of tweets regarding Covid-19. This project sparked my interest because I lived through the whole pandemic and have seen how panic in 2020 has turned into nonchalant, casual mentions of Covid-19 in 2022.  I wanted to revisit the early days of the pandemic and see what people were most concerned about by checking their tweets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627c25c-dbd2-4d23-a3ac-0ff7e275d86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f198d-55ac-4d1b-933f-baf000dd3170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in data\n",
    "df2 = pd.read_csv('Corona_NLP_test.csv')\n",
    "df3 = pd.read_csv('Corona_NLP_train.csv', encoding='latin-1')\n",
    "df_comb = pd.concat([df2, df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3021c00-91c1-4987-af89-7a5a5730d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to make sure all dfs combined correctly and checking what data looks like\n",
    "print(df_comb.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)\n",
    "df_comb.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8cd5f-ba25-47df-99d5-d146f23dad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming df to simply 'df'\n",
    "df= df_comb\n",
    "#35977 after re\n",
    "#8 after min_df =.1\n",
    "#191 after min_df = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ad9dc-f10a-41b2-b5d9-a4130941e11f",
   "metadata": {},
   "source": [
    "for x in df['OriginalTweet']:\n",
    "    x = re.sub('@\\S+', ' ', x)\n",
    "    x = re.sub('https*\\S+', ' ', x)\n",
    "    x = re.sub('#\\S+', ' ', x)\n",
    "    x = re.sub(\"\\'\\w+\", '', x)\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub('\\s{2,}', ' ', x)\n",
    "    #x = re.sub('/^[a-zA-Z0-9\\s]*$/g', '', x)\n",
    "    df['OriginalTweet'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809da8de-fcc7-401a-af08-588aa82b6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning tweets using regex\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "def text_preproc(x):\n",
    "    \n",
    "    x = x.lower()\n",
    "    x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    x = x.encode('ascii', 'ignore').decode()\n",
    "    x = re.sub(r'https*\\S+', ' ', x)\n",
    "    x = re.sub(r'@\\S+', ' ', x)\n",
    "    x = re.sub(r'#\\S+', ' ', x)\n",
    "    x = re.sub(r'\\'\\w+', '', x)\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    return x\n",
    "df['CleanTweet'] = df['OriginalTweet'].apply(text_preproc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff33e09-cec0-4ef0-90bc-86f31c1049e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tokenizing the tweets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "vectorizer = TfidfVectorizer(stop_words=my_stop_words, min_df = .01) \n",
    "X = vectorizer.fit_transform(df['CleanTweet'])\n",
    "tokens = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105266f-e39e-4971-8d99-5535f6f92817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING TOKENS WITH STEMMER AND MORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd56295-66fa-4731-8325-4eb38f8f58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74a5b3-82bd-4084-82ec-354e125de565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting stemmed words into a list called 'stem_words'\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "words = vectorizer.get_feature_names()\n",
    "stem_words = []\n",
    "for w in words:\n",
    "    y = snow_stemmer.stem(w)\n",
    "    stem_words.append(y)\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbae7b4-7a32-4f40-88a3-efdd646a129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing some LSA and Topic Modeling to try and isolate topics\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e5c62-f404-4ef3-b41d-c03268009b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note this one takes a bit to run, fitting the lsa model\n",
    "lsa = TruncatedSVD(n_components=2)\n",
    "lsa.fit(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744aebb1-7bbf-45fe-91c5-b216345c0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting x2 to use in kmeans graph display\n",
    "X2 = lsa.fit_transform(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5dbd0-473f-46cd-8ced-d206be463c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lsa.components_)\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03780f4-0f3d-4c39-b0a8-856b304d248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884a2da-5a23-46f3-8326-bcc21f4f7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45133d-e1c0-49d8-8c93-ec7b290caa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4441f-e3cc-4017-8f50-ce9caa51f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218e3b2-e9e9-49b6-806a-7de66a68d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.fit(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211cc83-56e6-45ad-a7fb-a93bee4e7e0b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc77b80-73ac-4579-b3a0-578f277a0344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying kmeans in graph using this guide, note df = X2, instead of pca transform i used lsa \n",
    "#https://www.askpython.com/python/examples/plot-k-means-clusters-python\n",
    "centroids = km.cluster_centers_\n",
    "X2 = lsa.fit_transform(tokens)\n",
    "label = km.fit_predict(X2)\n",
    "u_labels = np.unique(label)\n",
    "for i in u_labels:\n",
    "    plt.scatter(X2[label == i, 0], X2[label == i, 1], label = i)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], s = 80, color = 'k')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9fc71-9819-4e28-b96a-0f3fcdcebbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing X_test dimensions (cols) using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6093f4bc-3708-4c82-81af-068cff064d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X.toarray()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28b671-3a85-4663-ab07-401ff55bf8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_test))\n",
    "print(type(tokens))\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93189943-87fb-4613-af8b-24b2abd0e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to reduce X_test from (44944,191)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_PCA = pca.fit_transform(X_test)\n",
    "X_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cac649-5f59-4a6a-97eb-b79f873983c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "trying to plot the data\n",
    "'''\n",
    "xs, ys = X_PCA[:,0], X_PCA[:,1]\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "ax.margins(.05)\n",
    "ax.plot(xs, ys, marker='+', linestyle='', ms=3)\n",
    "ax.set_aspect('auto')\n",
    "\n",
    "plt.show()\n",
    "#fig.savefig('tweets_after_PCA.png', dpi=fig.dpi) #to save the figure if you want "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a69f68-f263-4b08-8ca0-d6b2cc757170",
   "metadata": {},
   "source": [
    "#kmeans attempt \n",
    "\n",
    "centroids = km.cluster_centers_\n",
    "label = km.fit_predict(X_PCA)\n",
    "u_labels = np.unique(label)\n",
    "for i in u_labels:\n",
    "    plt.scatter(X_PCA[label == i, 0], X_PCA[label == i, 1], label = i)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], s = 80, color = 'k')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9dbf4-3408-492c-b322-b6ae61f0cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans attempt 2\n",
    "km2 = KMeans(n_clusters=8, random_state=1)\n",
    "km2.fit(tokens)\n",
    "\n",
    "centroids2 = km2.cluster_centers_\n",
    "label2 = km2.fit_predict(X_PCA)\n",
    "u_labels2 = np.unique(label2)\n",
    "for i in u_labels2:\n",
    "    fig = plt.scatter(X_PCA[label2 == i, 0], X_PCA[label2 == i, 1], label = i)\n",
    "plt.scatter(centroids2[:,0], centroids2[:,1], s = .001, color = 'k')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#fig.figure.savefig('kmeans_8k.png') #to save the image if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809198fe-509f-4366-a24e-a9e216c7cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb[km.labels_==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec73a29d-6640-4294-813e-1b4c6cbf71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_track_one = pca.inverse_transform(X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d174ad-3c0c-475e-b5cd-53008f080243",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(back_track_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d244e-c1b9-41fd-8e1a-79a68d4534fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_track_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6e8c3-ecda-46c4-aa16-62556b16a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at token martix in df for cluster = i\n",
    "df_clust0 = tokens[km2.labels_==0]\n",
    "df_clust1 = tokens[km2.labels_==1]\n",
    "df_clust2 = tokens[km2.labels_==2]\n",
    "df_clust3 = tokens[km2.labels_==3]\n",
    "df_clust4 = tokens[km2.labels_==4]\n",
    "df_clust5 = tokens[km2.labels_==5]\n",
    "df_clust6 = tokens[km2.labels_==6]\n",
    "df_clust7 = tokens[km2.labels_==7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605cc06f-a184-4912-93e9-e6713bd0fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust0_words = df_clust0.sum() #supermarket shortages with words: sanitizer, panic, toilet, paper, buying, food\n",
    "clust1_words = df_clust1.sum() #changes since covid-19 with words: covid-19, consumer, prices, pandemic, online, shopping, impact, oil, crisis\n",
    "clust2_words = df_clust2.sum() #people concerned about going to grocery store: store, grocery, workers, people, 'just like going', stores, employees\n",
    "clust3_words = df_clust3.sum() #\n",
    "clust4_words = df_clust4.sum()\n",
    "clust5_words = df_clust5.sum()\n",
    "clust6_words = df_clust6.sum()\n",
    "clust7_words = df_clust7.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef824d57-3614-47f1-9611-cc132004b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing 10 most common words in each cluster based on tfidf value\n",
    "print('Cluster0 Words' + '\\n',  clust0_words.sort_values(ascending=False)[:10], '\\n')\n",
    "print('Cluster1 Words' + '\\n',  clust1_words.sort_values(ascending=False)[:10], '\\n')\n",
    "print('Cluster2 Words' + '\\n',  clust2_words.sort_values(ascending=False)[:10], '\\n')\n",
    "print('Cluster3 Words' + '\\n',  clust3_words.sort_values(ascending=False)[:10], '\\n')\n",
    "print('Cluster4 Words' + '\\n',  clust4_words.sort_values(ascending=False)[:10], '\\n')\n",
    "print('Cluster5 Words' + '\\n',  clust5_words.sort_values(ascending=False)[:10], '\\n')\n",
    "print('Cluster6 Words' + '\\n',  clust6_words.sort_values(ascending=False)[:10], '\\n')\n",
    "print('Cluster7 Words' + '\\n',  clust7_words.sort_values(ascending=False)[:10], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49782500-3c87-46ea-8559-1d07482ce15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with different number of clusters\n",
    "#trying to use inertia/elbow plot with km3 as our tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9e350-19ad-4af7-9b6c-94926a49c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determining inertia of kmeans clusters (higher = worse)\n",
    "inertia = []\n",
    "\n",
    "for num_clusters in range(1,20):\n",
    "    km3 = KMeans(n_clusters=num_clusters, random_state=1)\n",
    "    km3.fit(tokens)\n",
    "    inertia.append(km3.inertia_)\n",
    "inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5ac10-22e5-49ad-8063-d24aa6350fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting inertia (y) to number of clusters in kmeans (x)\n",
    "inertia_plot = plt.plot(inertia)\n",
    "\n",
    "plt.xlabel('Num Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.savefig('inertia_plot.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39085918-027e-43b3-abc0-0f148b2efa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bda562-23a3-403c-8800-00c0ca1eba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting sil_score and inertia in lists for easy viewing, checking for a range of clusters to determine the best cluster size (k) to use\n",
    "\n",
    "sil_score = []\n",
    "inertia = []\n",
    "\n",
    "for num_clusters in range(2,11):\n",
    "    km3 = KMeans(n_clusters=num_clusters, random_state=11)\n",
    "    km3.fit(tokens)\n",
    "    inertia.append(km3.inertia_)\n",
    "    sil_score.append(silhouette_score(tokens, km3.labels_, metric='euclidean'))\n",
    "\n",
    "print(inertia)\n",
    "print(sil_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d26939-143d-4e5b-9ea7-1ceb60b76c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying a DBSCAN cluster to see if it clusters in a better way than kmeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from random import randint\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff3a0b-d637-4ada-be0e-e7f7b1189ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#scaling X_PCA to X_DB\n",
    "X_DB = X_PCA\n",
    "X_DB = StandardScaler().fit_transform(X_DB)\n",
    "plt.scatter(X_DB[:, 0], X_DB[:, 1])\n",
    "\n",
    "DB_cluster = DBSCAN(eps=.1, min_samples=55).fit(X_DB)\n",
    "y_pred = DB_cluster.labels_\n",
    "#colors=np.array(['red', 'orange', 'yellow', 'green', 'blue', 'purple', 'pink', 'salmon', 'olive', 'aqua', 'crimson', 'mediumslateblue', 'steelblue', 'black'])\n",
    "colors = []\n",
    "for i in range(0, 50):\n",
    "    colors.append('#%06X' % randint(0, 0xFFFFFF))\n",
    "colors = np.array(colors)\n",
    "plt.scatter(X_DB[:, 0], X_DB[:, 1], color=colors[y_pred])\n",
    "\n",
    "#checking how many clusters/labels the DBSCAN has because of tuning parameters\n",
    "np.unique(DB_cluster.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c22db-a6c6-4a7e-9285-9758d073269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking most common words for DBACAN clusters... it seems like 15 clusters may be best... but there is only 2-3 main topics and a lot of noise\n",
    "df_clust0 = tokens[DB_cluster.labels_==0]\n",
    "df_clust1 = tokens[DB_cluster.labels_==1]\n",
    "df_clust2 = tokens[DB_cluster.labels_==2]\n",
    "df_clust3 = tokens[DB_cluster.labels_==3]\n",
    "df_clust4 = tokens[DB_cluster.labels_==4]\n",
    "df_clust5 = tokens[DB_cluster.labels_==5]\n",
    "#df_clust6 = tokens[DB_cluster.labels_==6]\n",
    "#df_clust7 = tokens[DB_cluster.labels_==7]\n",
    "\n",
    "clust0_words = df_clust0.sum() #\n",
    "clust1_words = df_clust1.sum() #\n",
    "clust2_words = df_clust2.sum() #\n",
    "clust3_words = df_clust3.sum() #\n",
    "clust4_words = df_clust4.sum()\n",
    "clust5_words = df_clust5.sum()\n",
    "#clust6_words = df_clust6.sum()\n",
    "#clust7_words = df_clust7.sum()\n",
    "\n",
    "\n",
    "\n",
    "print('Cluster0 Words' + '\\n',  clust0_words.sort_values(ascending=False)[:10], '\\n')\n",
    "print('Cluster1 Words' + '\\n',  clust1_words.sort_values(ascending=False)[:10], '\\n')\n",
    "print('Cluster2 Words' + '\\n',  clust2_words.sort_values(ascending=False)[:10], '\\n')\n",
    "print('Cluster3 Words' + '\\n',  clust3_words.sort_values(ascending=False)[:10], '\\n')\n",
    "print('Cluster4 Words' + '\\n',  clust4_words.sort_values(ascending=False)[:10], '\\n')\n",
    "print('Cluster5 Words' + '\\n',  clust5_words.sort_values(ascending=False)[:10], '\\n')\n",
    "#print('Cluster6 Words' + '\\n',  clust6_words.sort_values(ascending=False)[:10], '\\n')\n",
    "#print('Cluster7 Words' + '\\n',  clust7_words.sort_values(ascending=False)[:10], '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b49eb8-e45a-457a-bf0f-3db87ce6cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the most used words from the whole tweet dataset, not just a single cluster\n",
    "most_pop_words_overall = tokens.sum()\n",
    "most_pop_words_overall.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9fca6-c6d8-4d7d-8b7c-9575a685b682",
   "metadata": {},
   "source": [
    "#gathering some stuff for the presentation\n",
    "\n",
    "proj_tweet = []\n",
    "proj_tweet.append(df['OriginalTweet'].head(1))\n",
    "\n",
    "test_tweet = df['OriginalTweet'].iloc[1].split()\n",
    "test_tweet=' '.join(map(str, test_tweet))\n",
    "print(test_tweet)\n",
    "\n",
    "test_clean_tweet = df['CleanTweet'].iloc[1].split()\n",
    "test_clean_tweet=' '.join(map(str,test_clean_tweet))\n",
    "print(test_clean_tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
